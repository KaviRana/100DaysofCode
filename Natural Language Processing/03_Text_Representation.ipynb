{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) involves converting text documents into numerical vectors by counting the occurrences of each word in the document, disregarding grammar and word order. The resulting vector represents the frequency distribution of words in the document, forming a \"bag\" of words without considering their sequence or structure. BoW is commonly used for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW disregards the order and structure of words in the text and focuses solely on the frequency of individual words. The idea is to treat each document as an unordered \"bag\" of words, which allows us to represent text in a format suitable for machine learning tasks.By representing text as a frequency distribution of words, BoW captures the presence and importance of words in a document. It enables us to measure the similarity between documents, classify texts into different categories, perform sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a corpus of four text documents. We use the CountVectorizer from scikit-learn to convert the text into a bag of words representation. The fit_transform method creates a sparse matrix where each row corresponds to a document, and each column represents a unique word in the corpus. The cell values indicate the frequency of each word in the respective document.\n",
    "\n",
    "The get_feature_names_out() method returns the list of unique words in the corpus, which forms the vocabulary. The BoW matrix and the vocabulary are then printed for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to create the BoW representation\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the vocabulary (list of unique words) from the vectorizer\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the BoW representation and the vocabulary\n",
    "print(vocabulary)\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N grams \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are contiguous sequences of N items (words, characters, or tokens) extracted from a text or sentence. This is used to represent the frequency and relationships between words in a given piece of text.The core intuition behind using N-grams is to capture the local context and relationships between words within a text. By considering sequences of N items (words, characters, or tokens) together, N-grams provide a more comprehensive representation of the text compared to individual words (unigrams). This can be particularly useful in capturing phrases, collocations, and syntactic patterns in the language.\n",
    "\n",
    "For example, consider the sentence: \"I love to code.\" Here are some examples of different N-grams for this sentence:\n",
    "\n",
    "1-grams (unigrams): [\"I\", \"love\", \"to\", \"code\"] = Bag of Words!!!\n",
    "\n",
    "2-grams (bigrams): [\"I love\", \"love to\", \"to code\"]\n",
    "\n",
    "3-grams (trigrams): [\"I love to\", \"love to code\"]\n",
    "\n",
    "4-grams (fourgrams): [\"I love to code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams for document 1: ['This is', 'is the', 'the first', 'first document', 'document .']\n",
      "N-grams for document 2: ['This document', 'document is', 'is the', 'the second', 'second document', 'document .']\n",
      "N-grams for document 3: ['And this', 'this is', 'is the', 'the third', 'third one', 'one .']\n",
      "N-grams for document 4: ['Is this', 'this the', 'the first', 'first document', 'document ?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Function to generate N-grams for the entire corpus\n",
    "def generate_ngrams_for_corpus(corpus, n):\n",
    "    n_grams_corpus = []\n",
    "    for doc in corpus:\n",
    "        n_grams = ngrams(doc, n)\n",
    "        n_grams_doc = [' '.join(grams) for grams in n_grams]\n",
    "        n_grams_corpus.append(n_grams_doc)\n",
    "    return n_grams_corpus\n",
    "\n",
    "# Generate and store N-grams for the corpus\n",
    "N = 2\n",
    "n_grams_corpus = generate_ngrams_for_corpus(tokenized_corpus, N)\n",
    "\n",
    "# Print the N-grams for each document in the corpus\n",
    "for i, doc_ngrams in enumerate(n_grams_corpus):\n",
    "    print(f\"N-grams for document {i + 1}: {doc_ngrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF (Term Frequency-Inverse Document Frequency)\n",
    "TFIDF  is a numerical representation used to measure a word's importance in a document relative to a collection of documents. The core intuition is to prioritize words that are frequent in the document (TF) but rare across the corpus (IDF), making them more discriminative and informative for text analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for the TFIDF vectoriser \n",
    "Tokenization: Break down input documents into individual words (tokens).\n",
    "\n",
    "Counting: Count the frequency of each word in each document, creating a document-term matrix.\n",
    "\n",
    "Term Frequency (TF) Calculation: (frequency of each term in a document ) / total number of words in that document. It gives the probability of a word. \n",
    "\n",
    "Inverse Document Frequency (IDF) Calculation: It measures the rarity of each term across the entire corpus. loge(total number of documents in a corpus/the number of documents containing the term) + 1. The effect of adding 1 is to avoid any circumstance where the log value becomes zero since we have to multiply in the next term. \n",
    "\n",
    "TF-IDF Score Calculation: by multiplying its TF and IDF scores, resulting in a TF-IDF matrix representing the importance of words in the context of the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to get the TF-IDF matrix\n",
    "tfidf_matrix = Tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to an array for better visualization (optional)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
